{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the .mat file\n",
    "mat_data = loadmat(\"Moth_data.mat\")\n",
    "\n",
    "# Concatenate data from all variables\n",
    "data = []\n",
    "for key in mat_data:\n",
    "    if \"__\" not in key and \"readme\" not in key:\n",
    "        data.append(mat_data[key])\n",
    "\n",
    "data = np.concatenate(data)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Reshape data to have the format (samples, time_steps, features)\n",
    "data_reshaped = data_normalized.reshape(\n",
    "    data_normalized.shape[0], data_normalized.shape[1], 1)\n",
    "\n",
    "# Generate labels for the data\n",
    "labels = [i // 2 for i in range(12)] * (data.shape[0] // 12)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_reshaped, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = tf.keras.models.load_model(\"best_model.h5\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and display contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the .mat file\n",
    "mat_data = loadmat(\"Moth_data.mat\")\n",
    "\n",
    "# Print the keys (variable names) present in the file\n",
    "print(\"Keys (variable names) in the .mat file:\")\n",
    "for key in mat_data:\n",
    "    print(key)\n",
    "\n",
    "# Access and display the contents of a specific variable\n",
    "# Replace this with the actual variable name\n",
    "variable_name = \"your_variable_name_here\"\n",
    "if variable_name in mat_data:\n",
    "    variable_data = mat_data[variable_name]\n",
    "    print(f\"Contents of '{variable_name}':\")\n",
    "    print(variable_data)\n",
    "else:\n",
    "    print(f\"Variable '{variable_name}' not found in the .mat file.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the .mat file\n",
    "mat_data = loadmat(\"Moth_data.mat\")\n",
    "\n",
    "# Remove unnecessary keys\n",
    "mat_data.pop(\"__header__\", None)\n",
    "mat_data.pop(\"__version__\", None)\n",
    "mat_data.pop(\"__globals__\", None)\n",
    "\n",
    "# Print the keys (variable names) present in the file\n",
    "print(\"Keys (variable names) in the .mat file:\")\n",
    "for key in mat_data:\n",
    "    print(key)\n",
    "\n",
    "# Create a dictionary to store the data for each variable\n",
    "data_dict = {}\n",
    "\n",
    "# Access and store the contents of all variables\n",
    "for variable_name in mat_data.keys():\n",
    "    variable_data = mat_data[variable_name]\n",
    "    data_dict[variable_name] = variable_data\n",
    "    print(f\"Contents of '{variable_name}':\")\n",
    "    print(variable_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the data (replace 'data_dict' with your actual data)\n",
    "data = np.array([data_dict[key] for key in data_dict])\n",
    "# Assumes the data is 1-dimensional and stacks it along the time axis\n",
    "X = np.stack(data, axis=0)\n",
    "y = ...  # You need to provide labels for the data\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, input_shape=(\n",
    "        X_train.shape[1], 1), return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(y.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(\n",
    "    X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate(X_val, y_val)\n",
    "print(\"Validation loss:\", evaluation[0])\n",
    "print(\"Validation accuracy:\", evaluation[1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def reshape_data(data):\n",
    "    # Reshape the data to have the format (samples, time_steps, features)\n",
    "    return data.reshape(data.shape[0], data.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_hot_encode_labels(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(labels)\n",
    "    return to_categorical(integer_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# Assuming X is your data and y are your labels\n",
    "X_normalized = normalize_data(X)\n",
    "X_reshaped = reshape_data(X_normalized)\n",
    "y_one_hot = one_hot_encode_labels(y)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
